---
- name: S3 bucket creation
  collections:
    - amazon.aws
    - community.general
  module_defaults:
    group/aws:
      access_key: '{{ aws_access_key }}'
      secret_key: '{{ aws_secret_key }}'
      session_token: '{{ security_token | default(omit) }}'
      region: '{{ aws_region }}'
  block:
    # ============================================================
    - name: Create simple s3_bucket
      s3_bucket:
        name: "{{ test_bucket }}"
        state: present
        public_access:
          block_public_acls: false
        object_ownership: BucketOwnerPreferred
      register: output

    - assert:
        that:
          - output.changed
          - output.name == "{{ test_bucket }}"
          - not output.requester_pays
    # ============================================================
    - name: Prepare fixtures folder
      file:
        path: "{{ output_dir }}/s3_sync"
        state: directory
        mode: '0755'

    - name: Prepare files to sync
      copy:
        src: "{{ item }}"
        dest: "{{ output_dir }}/s3_sync/{{ item }}"
        mode: preserve
      with_items:
      - test1.txt
      - test2.yml
      - test3.json

    - name: Prepare file with size bigger than chunk size
      shell: |
        dd if=/dev/zero of=test4.txt bs=1M count=10
      args:
        chdir: "{{ output_dir }}/s3_sync"

    - name: Sync files with remote bucket
      s3_sync:
        bucket: "{{ test_bucket }}"
        file_root: "{{ output_dir }}/s3_sync"
      register: output
    - assert:
        that:
          - output is changed

    # ============================================================
    - name: Create a second s3_bucket
      s3_bucket:
        name: "{{ test_bucket_2 }}"
        state: present
      register: output

    - assert:
        that:
          - output.changed
          - output.name == "{{ test_bucket_2 }}"
          - not output.requester_pays

    - name: Sync files with remote bucket using glacier storage class
      s3_sync:
        bucket: "{{ test_bucket_2 }}"
        file_root: "{{ output_dir }}/s3_sync"
        storage_class: "GLACIER"
      register: output

    - assert:
        that:
          - output is changed

    # ============================================================

    - name: Sync files already present
      s3_sync:
        bucket: "{{ test_bucket }}"
        file_root: "{{ output_dir }}/s3_sync"
      register: output
    - assert:
        that:
          - output is not changed

    # ============================================================
    - name: Sync files with etag calculation
      s3_sync:
        bucket: "{{ test_bucket }}"
        file_root: "{{ output_dir }}/s3_sync"
        file_change_strategy: checksum
      register: output
    - assert:
        that:
          - output is not changed

    # ============================================================
    - name: Create a third s3_bucket
      s3_bucket:
        name: "{{ test_bucket_3 }}"
        state: present
      register: output

    - assert:
        that:
          - output.changed
          - output.name == "{{ test_bucket_3 }}"
          - not output.requester_pays

    - name: Sync individual file with remote bucket
      s3_sync:
        bucket: "{{ test_bucket_3 }}"
        file_root: "{{ output_dir }}/s3_sync/test1.txt"
      register: output
    - assert:
        that:
          - output is changed

    # ============================================================
    # DOCUMENTATION EXAMPLES
    # ============================================================
    - name: all the options
      s3_sync:
        bucket: "{{ test_bucket }}"
        file_root: "{{ output_dir }}/s3_sync"
        mime_map:
          .yml: application/text
          .json: application/text
        key_prefix: config_files/web
        file_change_strategy: force
        permission: public-read
        cache_control: "public, max-age=31536000"
        include: "*"
        exclude: "*.txt,.*"
      register: output

    - assert:
        that:
          - output is changed
          - '"uploads" in output'
          - '"filelist_initial" in output'
          - '"filelist_local_etag" in output'
          - '"filelist_s3" in output'
          - '"filelist_typed" in output'
          - '"filelist_actionable" in output'

  always:

    - name: Empty all buckets before deleting
      block:
        - name: list test_bucket objects
          s3_object:
            bucket: "{{ test_bucket }}"
            mode: list
          register: objects
          ignore_errors: true

        - name: remove objects from test_bucket
          s3_object:
            bucket: "{{ test_bucket }}"
            mode: delobj
            object: "{{ obj }}"
          with_items: "{{ objects.s3_keys }}"
          loop_control:
            loop_var: obj
          ignore_errors: true

        - name: list test_bucket_2 objects
          s3_object:
            bucket: "{{ test_bucket_2 }}"
            mode: list
          register: objects
          ignore_errors: true

        - name: remove objects from test_bucket_2
          s3_object:
            bucket: "{{ test_bucket_2 }}"
            mode: delobj
            object: "{{ obj }}"
          with_items: "{{ objects.s3_keys }}"
          loop_control:
            loop_var: obj
          ignore_errors: true

        - name: list test_bucket_3 objects
          s3_object:
            bucket: "{{ test_bucket_3 }}"
            mode: list
          register: objects
          ignore_errors: true

        - name: remove objects from test_bucket_3
          s3_object:
            bucket: "{{ test_bucket_3 }}"
            mode: delobj
            object: "{{ obj }}"
          with_items: "{{ objects.s3_keys }}"
          loop_control:
            loop_var: obj
          ignore_errors: true

    - name: Ensure all buckets are deleted
      s3_bucket:
        name: "{{item}}"
        state: absent
        force: true
      ignore_errors: yes
      with_items:
        - "{{ test_bucket }}"
        - "{{ test_bucket_2 }}"
        - "{{ test_bucket_3 }}"
